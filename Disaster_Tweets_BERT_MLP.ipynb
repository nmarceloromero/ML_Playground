{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d7555d",
   "metadata": {},
   "source": [
    "# Disaster Tweets Classification\n",
    "This notebook presents a task to perform classification of tweets (we need to define if the tweet is about a natural disaster or not). For this purpose, the BERT pre-trained network is used as feature extractor, and an MLP is trained and used as the classifier. The dataset used in this work is from the following Kaggle competition: https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26791473",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab7aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- IMPORTS --------------- #\n",
    "import torch\n",
    "import transformers\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch  import nn\n",
    "from torch  import optim\n",
    "from string import punctuation\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6aea26",
   "metadata": {},
   "source": [
    "## Define relevant file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76b6663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test file names.\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b996e154",
   "metadata": {},
   "source": [
    "## Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6ad69c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAABECAYAAADHuCM8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABoElEQVR4nO3XPUoDURiF4RszonEs/Ak2YmNjQBBsXITLcg+WLsUFpNFVBFKJiLVja2FSeXOP4Xnarzm3eZkZDcMwFIAQO60HAPwkSkAUUQKiiBIQRZSAKN2649XNrJR+vKktG7fTbXeT+2639YRqJt1e6wlV7XcHrSdU9fH2Webz+a+3tVEq/biM7o9rbIrQn0xaT6jq7vSi9YRqrqeXrSdUNZvetp5Q1ePD08rbdn8qAP+OKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFECoogSEEWUgCiiBEQRJSCKKAFRRAmIIkpAFFEConTrjkdfh+X89WxTW/hjy/LeekI1y/LSekJVz1v+vsVisfI2GoZh2OAWgLX8vgFRRAmIIkpAFFECoogSEOUbYQgb381NK8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAABECAYAAADHuCM8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABd0lEQVR4nO3XsY3iYBhF0X+MM8ANUAD9l0EhRK6AyJtusIM28fgOOif9ZOlZlq7kr23btgEQMR09AOBvogSkiBKQIkpAiigBKfO74/1+H9P0ud06nU5HT9iVb/d7ffK3G2OMeZ7H4/H49+3dg9M0jfP5vMuogsvlcvSEXX3y+12v16Mn7GpZlqMn7Gpd129vn51j4NcRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlImd8dl2UZt9vtp7bAf3u9XkdP2NW6rkdP2NXz+fz29rVt2/aDWwDe8vsGpIgSkCJKQIooASmiBKT8AXCrF9zAAMp7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAABECAYAAADHuCM8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABh0lEQVR4nO3XMW7UQACGUXtjUmxCEzrSIyFxhFyTQ+0dKNLsBaBFGloK4s7rD+e9dpp/ZOuTZh5jjAkg4rT3AIC/iRKQIkpAiigBKaIEpCxrh1++fpvGcr7Vlps7nVav/9+b7+72nrCZ+ejf7uD3+zj/nC6Xyz/PVm8+lvP0+/PLJqMK7h8/7T1hU8vj094TNnP/cOxv9+Hg/+bzj+9vnnm+ASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpy9rh03menh9ebzRlB+N17wXb+rX3gA0d+W7vwPV6ffNsHmOMG24BWOX5BqSIEpAiSkCKKAEpogSk/AEb2xmW863vNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define color palettes.\n",
    "greens = ['#27823b','#339444','#69a761','#94bb83','#bccfb4']\n",
    "darks  = ['#333333','#3f3f3f','#4b4b4b','#585858','#666666']\n",
    "blues  = ['#204671','#265284','#2b5e96','#3169a9','#3675bc']\n",
    "\n",
    "cmap_greens = sns.color_palette(greens)\n",
    "cmap_darks = sns.color_palette(darks)\n",
    "cmap_blues = sns.color_palette(blues)\n",
    "sns.set_palette(cmap_greens)\n",
    "\n",
    "# Show the palettes.\n",
    "sns.palplot(cmap_greens)\n",
    "sns.palplot(cmap_darks)\n",
    "sns.palplot(cmap_blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6acef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for the figures.\n",
    "sns.set_style('white')\n",
    "plt.rcParams['font.family'] = 'monospace'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be32998b",
   "metadata": {},
   "source": [
    "### Load BERT tokenizer and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faecd12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61d92c52fdf4f41916c98e891c9abab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81abf44f044f4fd0b5f50fe775c58112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf81303161a4876a9788fb1ef8c43b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d45391f00a246c2bf26abf2541e46ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba7168b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'it', 'is', 'raining', 'somewhere', 'else', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Sample to test things.\n",
    "sample = 'It is raining somewhere else.'\n",
    "\n",
    "# Test BERT tokenizer.\n",
    "marked_text = \"[CLS] \" + sample + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e015464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to obtain the vocab IDs of the tokenized sentence and the segment IDs vector of the sentence.\n",
    "def tokenize_and_segment(sentence, preprocess=False):\n",
    "    if preprocess:\n",
    "        sentence = ''.join([c for c in sentence if c not in punctuation]).lower()\n",
    "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text) # A vector of 1s.\n",
    "    return indexed_tokens, segments_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff193661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:\n",
      " [101, 2009, 2003, 24057, 4873, 2842, 102]\n",
      "\n",
      "Segment IDs vector:\n",
      " [1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Test the method.\n",
    "indexed_tokens, segments_ids = tokenize_and_segment(sample, preprocess=True)\n",
    "print('Tokenized sentence:\\n',indexed_tokens)\n",
    "print('\\nSegment IDs vector:\\n',segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f576c",
   "metadata": {},
   "source": [
    "Why do we need Segment IDs? The post https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#what-is-bert explains everything smoothly. Most of the BERT-related tasks of this notebook are based on that tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ead104",
   "metadata": {},
   "source": [
    "### Load pre-trained BERT and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff30f08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d10c0a045a041e5b9aa4ec5d61f1f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c5caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to apply the BERT model on one sentence composed of indexes of tokens and segments.\n",
    "def get_hidden_states(bert_model, idx_tokens, idx_segments):\n",
    "    tokens_tensor   = torch.tensor([idx_tokens])\n",
    "    segments_tensor = torch.tensor([idx_segments])\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = torch.stack(outputs.hidden_states, dim=0) \n",
    "    return hidden_states # Output shape: [layers, batches, seq_len, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a59e84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BERT hidden states:  torch.Size([13, 1, 7, 768])\n",
      "Num of layers:  13\n",
      "Num batches:  1\n",
      "Sequence length:  7\n",
      "Num of hidden units:  768\n"
     ]
    }
   ],
   "source": [
    "# Apply BERT and analyse the hidden_states.\n",
    "hidden_bert = get_hidden_states(bert_model, indexed_tokens, segments_ids)\n",
    "print('Shape of BERT hidden states: ', hidden_bert.shape)\n",
    "print('Num of layers: ', hidden_bert.shape[0])\n",
    "print('Num batches: ', hidden_bert.shape[1])\n",
    "print('Sequence length: ', hidden_bert.shape[2])\n",
    "print('Num of hidden units: ', hidden_bert.shape[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818277c4",
   "metadata": {},
   "source": [
    "### Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f160a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the last N layers (last_layers) of each token producing a single N-length vector.\n",
    "def create_sentence_embeddings(hidden_states, layer_idx=-2):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "        hidden_states: output from the BERT model.\n",
    "        layer_idx: the layer to be used for obtaining the features (default=second to last layer).\n",
    "    '''\n",
    "    \n",
    "    # Remove the batch dim, since we work with individual sentences.\n",
    "    token_embeddings = torch.squeeze(hidden_states, dim=1) # Output shape: [layers, seq_len, features]\n",
    "    \n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2) # Output shape: [seq_len, layers, features]\n",
    "    \n",
    "    # Obtain average of features from all tokens.\n",
    "    sentence_embeddings = torch.mean(hidden_states[layer_idx][0], dim=0)\n",
    "    \n",
    "    # Return embeddings.\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2cc129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "feature_vector = create_sentence_embeddings(hidden_bert)\n",
    "print(feature_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd721b",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "Use the `create_sentence_embeddings` function as feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d226f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_feature_extractor(sentences, bert_model, preprocess=False, as_numpy=False, layer_idx=-2):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "        sentences: list of sentences.\n",
    "        bert_model: pre-trained BERT model.\n",
    "        as_numpy: return tensors as numpy or not. If False, then dataset[i] is a torch tensor.\n",
    "        \n",
    "    Output\n",
    "        dataset: a list with N tensors (torch or numpy) with features, with N=len(sentences).\n",
    "    '''\n",
    "    \n",
    "    # Our dataset.\n",
    "    dataset = []\n",
    "    \n",
    "    # Put the model in \"evaluation\" mode.\n",
    "    bert_model.eval()\n",
    "    \n",
    "    # Go through the sentences and get the features of each.\n",
    "    for sentence in sentences:\n",
    "        idx_tokens, idx_segments = tokenize_and_segment(sentence, preprocess)\n",
    "        hidden_bert = get_hidden_states(bert_model, idx_tokens, idx_segments)\n",
    "        feature_vector = create_sentence_embeddings(hidden_bert, layer_idx)\n",
    "        dataset.append(feature_vector.numpy() if as_numpy else feature_vector)\n",
    "    \n",
    "    # Return features list.\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3923c167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(768,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['It is dangerous to go alone, take this.', 'It is raining somewhere else.']\n",
    "dataset = bert_feature_extractor(sentences, bert_model, preprocess=True, as_numpy=True)\n",
    "print(len(dataset))\n",
    "print(dataset[0].shape)\n",
    "print(dataset[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae06d14",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e1fab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and see the first rows of the dataset.\n",
    "df = pd.read_csv(TRAIN_FILE, sep=\",\")\n",
    "df = df.drop(columns=['id', 'keyword', 'location'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbfb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_FILE, sep=\",\")\n",
    "df = df.drop(columns=['id', 'keyword', 'location'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e213ef",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a451980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the missing values.\n",
    "missing = df.isna().sum()\n",
    "missing = missing.sort_values(ascending=False)\n",
    "missing = missing[missing>0]\n",
    "missing = pd.DataFrame({'Attribute': missing.index, 'Count': missing.values})\n",
    "print('Number of missing values: %d' % len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7616061a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets get the counts of each class.\n",
    "counts_class = df.target.value_counts()\n",
    "counts_class = pd.DataFrame({'Class': counts_class.index, 'Count': counts_class.values})\n",
    "\n",
    "# Plot class distribution.\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Class count', fontweight='bold')\n",
    "ax = sns.barplot(data=counts_class, x='Class', y='Count')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['left'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False) \n",
    "ax.spines['bottom'].set_visible(True) \n",
    "\n",
    "for i in range(0, len(counts_class)):\n",
    "    ax.annotate(counts_class['Count'][i], \n",
    "                xy = (i, counts_class['Count'][i] - 500),\n",
    "                va='center', ha='center', color='#ffff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3d206",
   "metadata": {},
   "source": [
    "We can see that there is some degree of class imbalance, and we can tackle this through different approaches. However, since the difference is not that big, we will train the models using the entire dataset without any strategy to treat class imbalance. We will take one thing into account though, we will use F1-Score as metric to define how well our predictors perform instead of raw accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326089a",
   "metadata": {},
   "source": [
    "## Extract BERT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db087a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7613, 768)\n",
      "y_train shape: (7613,)\n"
     ]
    }
   ],
   "source": [
    "# Forward the train set to BERT to obtain the feature vectors for each sentence.\n",
    "X_train = bert_feature_extractor(sentences=df['text'], bert_model=bert_model, preprocess=True, as_numpy=True)\n",
    "y_train = df['target'].tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04015734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (3263, 768)\n"
     ]
    }
   ],
   "source": [
    "# Define a dataframe to contain results and add the predictions.\n",
    "df = pd.read_csv(TEST_FILE, sep=\",\")\n",
    "X_test = bert_feature_extractor(sentences=df['text'], bert_model=bert_model, preprocess=True, as_numpy=True)\n",
    "X_test = np.asarray(X_test)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac758ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump pre-processed train and test sets into numpy files.\n",
    "X_TRAIN_BERT_FILE = 'X_train_bert_feat_preprocessed.npy'\n",
    "Y_TRAIN_BERT_FILE = 'y_train_bert_feat_preprocessed.npy'\n",
    "X_TEST_BERT_FILE  = 'X_test_bert_feat_preprocessed.npy'\n",
    "np.save(X_TRAIN_BERT_FILE, X_train)\n",
    "np.save(Y_TRAIN_BERT_FILE, y_train)\n",
    "np.save(X_TEST_BERT_FILE,  X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734fc59",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now that we have our train set prepared, let's apply the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63790f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb635b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class of the model.\n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_units, activations, dropout=0.5):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # Check if the number of elements in num_units and activations must be the same.\n",
    "        assert len(num_units) == len(activations)\n",
    "        \n",
    "        # We can also check here if that all elements of num_units are \n",
    "        # two-dimensional, but let's trust the user ok?\n",
    "        \n",
    "        # Actually do the thing.\n",
    "        self.layers      = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        self.num_layers  = len(num_units) \n",
    "        for idx in range(self.num_layers): \n",
    "            self.layers.append(nn.Linear(num_units[idx][0], num_units[idx][1])) # Linear layers.\n",
    "            # Activation functions.\n",
    "            if activations[idx] == 'relu':\n",
    "                self.activations.append(nn.ReLU())\n",
    "            elif activations[idx] == 'sigmoid':\n",
    "                self.activations.append(nn.Sigmoid())\n",
    "            elif activations[idx] == 'softmax':\n",
    "                self.activations.append(nn.Softmax(dim=1))\n",
    "            else:\n",
    "                self.activations.append(None)\n",
    "        self.drop_layer = nn.Dropout(p=dropout) # Dropout layer.\n",
    "        self.init_weights() # Init weights.\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        for idx in range(self.num_layers):\n",
    "            self.layers[idx].weight.data.uniform_(-initrange, initrange)\n",
    "            self.layers[idx].bias.data.zero_()\n",
    "\n",
    "    def forward(self, X, dropout=0.0):\n",
    "        output = X\n",
    "        for idx in range(self.num_layers): # For each layer.\n",
    "            output = self.layers[idx](output) # Linear layers.\n",
    "            if self.activations[idx] != None: # Apply activation function if there is any.\n",
    "                output = self.activations[idx](output)\n",
    "            if idx != self.num_layers - 1: # Apply dropout if it is not the last layer.\n",
    "                output = self.drop_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f35df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model for each batch.\n",
    "def train(model, optimizer, criterion, dataloader, epoch):\n",
    "    losses = []\n",
    "    total_acc, total_count, total_loss = 0, 0, 0\n",
    "    log_interval = 50\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted = model(x).view(model(x).shape[0])\n",
    "        loss = criterion(predicted, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # Gradient clipping.\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted.round() == y).sum().item()\n",
    "        total_loss += loss\n",
    "        total_count += y.size(0)\n",
    "        losses.append((loss / y.size(0)).view(-1).detach().numpy()[0])\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f} | loss {:8.5f}'\n",
    "                  .format(epoch, idx, len(dataloader), total_acc/total_count, total_loss/total_count))\n",
    "            total_acc, total_count, total_loss = 0, 0, 0\n",
    "    return losses # Return a list of the value of the loss at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524cf0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to forward the model and get the loss.\n",
    "def get_model_loss(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    total_count, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(dataloader):\n",
    "            predicted = model(x).view(model(x).shape[0])\n",
    "            loss = criterion(predicted, y)\n",
    "            total_count += y.size(0)\n",
    "            total_loss  += loss.item()\n",
    "    return total_loss/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83dad7",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ea09764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with features extracted with BERT.\n",
    "X_TRAIN_BERT_FILE = 'X_train_bert_feat_preprocessed.npy'\n",
    "Y_TRAIN_BERT_FILE = 'y_train_bert_feat_preprocessed.npy'\n",
    "X = np.load(X_TRAIN_BERT_FILE)\n",
    "y = np.load(Y_TRAIN_BERT_FILE)\n",
    "\n",
    "# Split dataset.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29158422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (7613, 768)\n",
      "Y shape:  (7613,)\n",
      "X_train shape:  (7232, 768)\n",
      "Y_train shape:  (7232,)\n",
      "X_val shape:  (381, 768)\n",
      "Y_val shape:  (381,)\n",
      "Validation instances of class 0: 217\n",
      "Validation instances of class 1: 164\n"
     ]
    }
   ],
   "source": [
    "# Show shapes of each dataset.\n",
    "print('X shape: ', X.shape)\n",
    "print('Y shape: ', y.shape)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('Y_train shape: ', y_train.shape)\n",
    "print('X_val shape: ', X_val.shape)\n",
    "print('Y_val shape: ', y_val.shape)\n",
    "\n",
    "## Show class distributions for the validation set.\n",
    "print('Validation instances of class 0: %d' % np.sum(y_val==0))\n",
    "print('Validation instances of class 1: %d' % np.sum(y_val==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b4bb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to torch tensors.\n",
    "# Train sets.\n",
    "X_train          = torch.Tensor(X_train)\n",
    "y_train          = torch.Tensor(y_train)\n",
    "train_dataset    = TensorDataset(X_train, y_train)\n",
    "\n",
    "# Validation sets.\n",
    "X_val          = torch.Tensor(X_val)\n",
    "y_val          = torch.Tensor(y_val)\n",
    "val_dataset    = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d5e056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters.\n",
    "BATCH_SIZE    = 64      # Batch size.\n",
    "EPOCHS        = 50      # Epochs.\n",
    "LEARNING_RATE = 0.0005  # Learning rate.\n",
    "DROPOUT       = 0.5     # Dropout.\n",
    "L2_LAMBDA     = 1e-4    # Lambda for L2 Regularization.\n",
    "\n",
    "# Model config.\n",
    "INPUT_SIZE  = 768 # The size of the BERT features.\n",
    "OUTPUT_SIZE = 1   # Binary classification, so the output is only one element.\n",
    "num_units = [[INPUT_SIZE, 512], [512, 256], [256, 128], [128, OUTPUT_SIZE]]\n",
    "activations = ['relu', 'relu', 'relu', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f46b146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init.\n",
    "model = Classifier(num_units=num_units, activations=activations).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6b3bc",
   "metadata": {},
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bbc8257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------- TRAINING MODEL --------------------#\n",
      "EPOCH 1 \n",
      "| epoch   1 |    50/  118 batches | accuracy    0.484 | loss  0.22982\n",
      "| epoch   1 |   100/  118 batches | accuracy    0.490 | loss  0.22602\n",
      "EPOCH 2 \n",
      "| epoch   2 |    50/  118 batches | accuracy    0.702 | loss  0.13173\n",
      "| epoch   2 |   100/  118 batches | accuracy    0.754 | loss  0.09992\n",
      "EPOCH 3 \n",
      "| epoch   3 |    50/  118 batches | accuracy    0.760 | loss  0.09690\n",
      "| epoch   3 |   100/  118 batches | accuracy    0.788 | loss  0.07948\n",
      "EPOCH 4 \n",
      "| epoch   4 |    50/  118 batches | accuracy    0.770 | loss  0.07287\n",
      "| epoch   4 |   100/  118 batches | accuracy    0.793 | loss  0.05135\n",
      "EPOCH 5 \n",
      "| epoch   5 |    50/  118 batches | accuracy    0.774 | loss  0.04519\n",
      "| epoch   5 |   100/  118 batches | accuracy    0.782 | loss  0.03092\n",
      "EPOCH 6 \n",
      "| epoch   6 |    50/  118 batches | accuracy    0.772 | loss  0.02593\n",
      "| epoch   6 |   100/  118 batches | accuracy    0.791 | loss  0.01805\n",
      "EPOCH 7 \n",
      "| epoch   7 |    50/  118 batches | accuracy    0.775 | loss  0.01636\n",
      "| epoch   7 |   100/  118 batches | accuracy    0.787 | loss  0.01311\n",
      "EPOCH 8 \n",
      "| epoch   8 |    50/  118 batches | accuracy    0.777 | loss  0.01188\n",
      "| epoch   8 |   100/  118 batches | accuracy    0.806 | loss  0.00995\n",
      "EPOCH 9 \n",
      "| epoch   9 |    50/  118 batches | accuracy    0.773 | loss  0.01166\n",
      "| epoch   9 |   100/  118 batches | accuracy    0.810 | loss  0.00851\n",
      "EPOCH 10 \n",
      "| epoch  10 |    50/  118 batches | accuracy    0.777 | loss  0.01009\n",
      "| epoch  10 |   100/  118 batches | accuracy    0.823 | loss  0.00709\n",
      "EPOCH 11 \n",
      "| epoch  11 |    50/  118 batches | accuracy    0.797 | loss  0.00815\n",
      "| epoch  11 |   100/  118 batches | accuracy    0.802 | loss  0.00805\n",
      "EPOCH 12 \n",
      "| epoch  12 |    50/  118 batches | accuracy    0.777 | loss  0.00924\n",
      "| epoch  12 |   100/  118 batches | accuracy    0.791 | loss  0.00801\n",
      "EPOCH 13 \n",
      "| epoch  13 |    50/  118 batches | accuracy    0.754 | loss  0.01044\n",
      "| epoch  13 |   100/  118 batches | accuracy    0.828 | loss  0.00626\n",
      "EPOCH 14 \n",
      "| epoch  14 |    50/  118 batches | accuracy    0.789 | loss  0.00847\n",
      "| epoch  14 |   100/  118 batches | accuracy    0.828 | loss  0.00657\n",
      "EPOCH 15 \n",
      "| epoch  15 |    50/  118 batches | accuracy    0.785 | loss  0.00853\n",
      "| epoch  15 |   100/  118 batches | accuracy    0.816 | loss  0.00664\n",
      "EPOCH 16 \n",
      "| epoch  16 |    50/  118 batches | accuracy    0.779 | loss  0.00835\n",
      "| epoch  16 |   100/  118 batches | accuracy    0.805 | loss  0.00671\n",
      "EPOCH 17 \n",
      "| epoch  17 |    50/  118 batches | accuracy    0.768 | loss  0.00899\n",
      "| epoch  17 |   100/  118 batches | accuracy    0.797 | loss  0.00733\n",
      "EPOCH 18 \n",
      "| epoch  18 |    50/  118 batches | accuracy    0.793 | loss  0.00775\n",
      "| epoch  18 |   100/  118 batches | accuracy    0.789 | loss  0.00784\n",
      "EPOCH 19 \n",
      "| epoch  19 |    50/  118 batches | accuracy    0.778 | loss  0.00839\n",
      "| epoch  19 |   100/  118 batches | accuracy    0.787 | loss  0.00717\n",
      "EPOCH 20 \n",
      "| epoch  20 |    50/  118 batches | accuracy    0.774 | loss  0.00787\n",
      "| epoch  20 |   100/  118 batches | accuracy    0.804 | loss  0.00719\n",
      "EPOCH 21 \n",
      "| epoch  21 |    50/  118 batches | accuracy    0.809 | loss  0.00690\n",
      "| epoch  21 |   100/  118 batches | accuracy    0.819 | loss  0.00618\n",
      "EPOCH 22 \n",
      "| epoch  22 |    50/  118 batches | accuracy    0.787 | loss  0.00779\n",
      "| epoch  22 |   100/  118 batches | accuracy    0.831 | loss  0.00624\n",
      "EPOCH 23 \n",
      "| epoch  23 |    50/  118 batches | accuracy    0.762 | loss  0.00854\n",
      "| epoch  23 |   100/  118 batches | accuracy    0.784 | loss  0.00729\n",
      "EPOCH 24 \n",
      "| epoch  24 |    50/  118 batches | accuracy    0.782 | loss  0.00737\n",
      "| epoch  24 |   100/  118 batches | accuracy    0.813 | loss  0.00683\n",
      "EPOCH 25 \n",
      "| epoch  25 |    50/  118 batches | accuracy    0.787 | loss  0.00802\n",
      "| epoch  25 |   100/  118 batches | accuracy    0.804 | loss  0.00693\n",
      "EPOCH 26 \n",
      "| epoch  26 |    50/  118 batches | accuracy    0.795 | loss  0.00727\n",
      "| epoch  26 |   100/  118 batches | accuracy    0.831 | loss  0.00607\n",
      "EPOCH 27 \n",
      "| epoch  27 |    50/  118 batches | accuracy    0.796 | loss  0.00703\n",
      "| epoch  27 |   100/  118 batches | accuracy    0.827 | loss  0.00625\n",
      "EPOCH 28 \n",
      "| epoch  28 |    50/  118 batches | accuracy    0.770 | loss  0.00779\n",
      "| epoch  28 |   100/  118 batches | accuracy    0.786 | loss  0.00768\n",
      "EPOCH 29 \n",
      "| epoch  29 |    50/  118 batches | accuracy    0.812 | loss  0.00673\n",
      "| epoch  29 |   100/  118 batches | accuracy    0.829 | loss  0.00614\n",
      "EPOCH 30 \n",
      "| epoch  30 |    50/  118 batches | accuracy    0.812 | loss  0.00648\n",
      "| epoch  30 |   100/  118 batches | accuracy    0.833 | loss  0.00604\n",
      "EPOCH 31 \n",
      "| epoch  31 |    50/  118 batches | accuracy    0.766 | loss  0.00820\n",
      "| epoch  31 |   100/  118 batches | accuracy    0.850 | loss  0.00555\n",
      "EPOCH 32 \n",
      "| epoch  32 |    50/  118 batches | accuracy    0.819 | loss  0.00653\n",
      "| epoch  32 |   100/  118 batches | accuracy    0.847 | loss  0.00572\n",
      "EPOCH 33 \n",
      "| epoch  33 |    50/  118 batches | accuracy    0.827 | loss  0.00627\n",
      "| epoch  33 |   100/  118 batches | accuracy    0.850 | loss  0.00553\n",
      "EPOCH 34 \n",
      "| epoch  34 |    50/  118 batches | accuracy    0.801 | loss  0.00720\n",
      "| epoch  34 |   100/  118 batches | accuracy    0.843 | loss  0.00586\n",
      "EPOCH 35 \n",
      "| epoch  35 |    50/  118 batches | accuracy    0.820 | loss  0.00629\n",
      "| epoch  35 |   100/  118 batches | accuracy    0.860 | loss  0.00517\n",
      "EPOCH 36 \n",
      "| epoch  36 |    50/  118 batches | accuracy    0.822 | loss  0.00622\n",
      "| epoch  36 |   100/  118 batches | accuracy    0.862 | loss  0.00515\n",
      "EPOCH 37 \n",
      "| epoch  37 |    50/  118 batches | accuracy    0.819 | loss  0.00631\n",
      "| epoch  37 |   100/  118 batches | accuracy    0.855 | loss  0.00538\n",
      "EPOCH 38 \n",
      "| epoch  38 |    50/  118 batches | accuracy    0.824 | loss  0.00623\n",
      "| epoch  38 |   100/  118 batches | accuracy    0.845 | loss  0.00577\n",
      "EPOCH 39 \n",
      "| epoch  39 |    50/  118 batches | accuracy    0.820 | loss  0.00632\n",
      "| epoch  39 |   100/  118 batches | accuracy    0.861 | loss  0.00511\n",
      "EPOCH 40 \n",
      "| epoch  40 |    50/  118 batches | accuracy    0.822 | loss  0.00655\n",
      "| epoch  40 |   100/  118 batches | accuracy    0.808 | loss  0.00693\n",
      "EPOCH 41 \n",
      "| epoch  41 |    50/  118 batches | accuracy    0.842 | loss  0.00558\n",
      "| epoch  41 |   100/  118 batches | accuracy    0.867 | loss  0.00498\n",
      "EPOCH 42 \n",
      "| epoch  42 |    50/  118 batches | accuracy    0.838 | loss  0.00582\n",
      "| epoch  42 |   100/  118 batches | accuracy    0.870 | loss  0.00490\n",
      "EPOCH 43 \n",
      "| epoch  43 |    50/  118 batches | accuracy    0.834 | loss  0.00608\n",
      "| epoch  43 |   100/  118 batches | accuracy    0.853 | loss  0.00521\n",
      "EPOCH 44 \n",
      "| epoch  44 |    50/  118 batches | accuracy    0.845 | loss  0.00552\n",
      "| epoch  44 |   100/  118 batches | accuracy    0.886 | loss  0.00445\n",
      "EPOCH 45 \n",
      "| epoch  45 |    50/  118 batches | accuracy    0.848 | loss  0.00545\n",
      "| epoch  45 |   100/  118 batches | accuracy    0.882 | loss  0.00449\n",
      "EPOCH 46 \n",
      "| epoch  46 |    50/  118 batches | accuracy    0.868 | loss  0.00492\n",
      "| epoch  46 |   100/  118 batches | accuracy    0.883 | loss  0.00433\n",
      "EPOCH 47 \n",
      "| epoch  47 |    50/  118 batches | accuracy    0.872 | loss  0.00476\n",
      "| epoch  47 |   100/  118 batches | accuracy    0.895 | loss  0.00410\n",
      "EPOCH 48 \n",
      "| epoch  48 |    50/  118 batches | accuracy    0.874 | loss  0.00473\n",
      "| epoch  48 |   100/  118 batches | accuracy    0.873 | loss  0.00466\n",
      "EPOCH 49 \n",
      "| epoch  49 |    50/  118 batches | accuracy    0.871 | loss  0.00477\n",
      "| epoch  49 |   100/  118 batches | accuracy    0.903 | loss  0.00376\n",
      "EPOCH 50 \n",
      "| epoch  50 |    50/  118 batches | accuracy    0.877 | loss  0.00462\n",
      "| epoch  50 |   100/  118 batches | accuracy    0.892 | loss  0.00416\n"
     ]
    }
   ],
   "source": [
    "# Set up dataloaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define loss function and optimizer.\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Train the network.\n",
    "train_losses_per_epoch = []\n",
    "val_losses_per_epoch   = []\n",
    "model.train()\n",
    "print('#' + '-' * 20 + ' TRAINING MODEL ' + '-' * 20 + '#')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print('EPOCH %d ' % epoch)\n",
    "    train_loss = train(model, optimizer, criterion, train_dataloader, epoch)\n",
    "    val_loss   = get_model_loss(model, criterion, val_dataloader)\n",
    "    train_losses_per_epoch.append(train_loss)\n",
    "    val_losses_per_epoch.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6478c1f8",
   "metadata": {},
   "source": [
    "Evolution of the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a43cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up data to be plotted.\n",
    "epoch       = []\n",
    "all_losses  = []\n",
    "train_loss_values = []\n",
    "for i in range(0, len(train_losses_per_epoch)): \n",
    "    all_losses.extend(train_losses_per_epoch[i]) # To plot all losses.\n",
    "    epoch.append(i*len(train_losses_per_epoch[i])) # To plot the average loss of each epoch.\n",
    "    train_loss_values.append(np.mean(train_losses_per_epoch[i]))\n",
    "    \n",
    "# Plot the figure.\n",
    "jeje = [i for i in range(0, len(val_losses_per_epoch))]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(jeje, train_loss_values, color=greens[0], linestyle='-', label='Train')\n",
    "plt.plot(jeje, val_losses_per_epoch, color=blues[4], linestyle='-', label='Validation')\n",
    "plt.legend(fontsize=8)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede614b",
   "metadata": {},
   "source": [
    "This looks nice. Let's evaluate the model now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496a416",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1fc231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to forward the trained model and write report.\n",
    "def get_model_predictions(model, dataset):\n",
    "    model.eval()\n",
    "    y_probs  = []\n",
    "    y_preds  = []\n",
    "    with torch.no_grad():\n",
    "        for idx, x in enumerate(dataset):\n",
    "            probs = model(x).numpy()\n",
    "            y_probs.append(probs[0])\n",
    "            y_preds.append(int(round(probs[0])))\n",
    "    return y_probs, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e9f58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with features extracted with BERT and define dataloader.\n",
    "X_TEST_BERT_FILE  = 'X_test_bert_feat_preprocessed.npy'\n",
    "X_test           = np.load(X_TEST_BERT_FILE)\n",
    "X_test           = torch.Tensor(X_test)\n",
    "\n",
    "# Forward the test dataset.\n",
    "y_probs, y_preds = get_model_predictions(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d106cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format results.\n",
    "df = pd.read_csv(TEST_FILE, sep=\",\")\n",
    "df = df.drop(columns=['text', 'keyword', 'location'])\n",
    "df['target'] = y_preds\n",
    "\n",
    "# Save the results and the best model.\n",
    "df.to_csv('bert_nn_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d3358",
   "metadata": {},
   "source": [
    "We do not have the ground truth labels since the dataset is from a Kaggle competition. However, this approach gives a score of around $0.81$ for the metric used in that competition. Other strategies could be tested in the future to improve this result, and also some attributes from the dataset that were not used in this notebook (keyword and location) could be considered, maybe they will allow to obtain better results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
