{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "Using the dataset in: https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "\n",
    "The task is to predict if a tweet is about a natural disaster or not. Therefore, it is a typical two-class classification problem. In this notebook, the problem is tackled through the use of pre-trained BERT model as feature extractor and a simple MLP that is trained to classify the tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# --------------- IMPORTS --------------- #\n",
    "import torch\n",
    "import transformers\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch  import nn\n",
    "from torch  import optim\n",
    "from string import punctuation\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Styles\n",
    "\n",
    "The Seaborn library is used to plot some graphs. Here we define the color palettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAABICAYAAABFhGj3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB00lEQVR4nO3YMWpUURiG4f86d0xiLIJOyqCkEmwMiCuwdCGuKWuyl4iNfToXcOxDArmQ4/lyeJ7yMgzfD8PLzCyttQJI9GL0AICHCBQQS6CAWAIFxBIoIJZAAbHWLS/ene7b/uyo15bhdru5e326vhw9oZvjdd7PZVXVyfpq9ISufv38fdtaO7/7fFOg9mdH9f77pycbleb1m5PRE7r68vZi9IRuPh4uR0/o6sPhavSErr5efftz3/O5vzIAz5pAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUECspbX2+Bcvy9+quuk3Z7hDVd2OHtHJzLdVue+5e9daO7/7cN34Jjettc9PNCjOsiw/Zr1v5tuq3DcrP/GAWAIFxNoaqOsuK3LMfN/Mt1W5b0qb/iQH+J/8xANiCRQQS6CAWAIFxBIoINY/bgg1aAuOtlMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAABICAYAAABFhGj3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABsUlEQVR4nO3YMWrDQBRF0ZEtUGO7iuvU7rNarSkbcJleXRYw6U0CEWQyz8M55SDE/wiukKZaawFIdOg9AMBPBAqIJVBALIECYgkUEEuggFjzrovnuS7L0mqW7g6HsXt9PB57j9CMZ/fctm3baq3Xx/NdgVqWpdxut7+bKszpdOo9QlMj73c+n3uP0NTlcuk9QlPrun58dz72awd4agIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbEECoglUEAsgQJiCRQQS6CAWAIFxBIoIJZAAbGmWuvvL56mz1LKvd043b2UUrbeQzQy8m6l2O/ZvdZar4+H886b3Gutb380UJxpmt5H3W/k3Uqx36h84gGxBAqItTdQa5Mpcoy838i7lWK/Ie36SQ7wn3ziAbEECoglUEAsgQJiCRQQ6wtjzzVvQVgDPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define color palettes.\n",
    "greens = ['#27823b','#339444','#69a761','#94bb83','#bccfb4']\n",
    "darks = ['#333333','#3f3f3f','#4b4b4b','#585858','#666666']\n",
    "cmap_greens = sns.color_palette(greens)\n",
    "cmap_darks = sns.color_palette(darks)\n",
    "sns.set_palette(cmap_greens)\n",
    "\n",
    "# Show the palettes.\n",
    "sns.palplot(cmap_greens)\n",
    "sns.palplot(cmap_darks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for the figures.\n",
    "sns.set_style('white')\n",
    "plt.rcParams['font.family'] = 'monospace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer\n",
    "\n",
    "Load the BERT tokenizer and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'it', 'is', 'raining', 'somewhere', 'else', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Sample to test things.\n",
    "sample = 'It is raining somewhere else.'\n",
    "\n",
    "# Test BERT tokenizer.\n",
    "marked_text = \"[CLS] \" + sample + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to obtain the vocab IDs of the tokenized sentence and the segment IDs vector of the sentence.\n",
    "def tokenize_and_segment(sentence, preprocess=False):\n",
    "    if preprocess:\n",
    "        sentence = ''.join([c for c in sentence if c not in punctuation]).lower()\n",
    "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text) # A vector of 1s.\n",
    "    return indexed_tokens, segments_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:\n",
      " [101, 2009, 2003, 24057, 4873, 2842, 102]\n",
      "\n",
      "Segment IDs vector:\n",
      " [1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Test the method.\n",
    "indexed_tokens, segments_ids = tokenize_and_segment(sample, preprocess=True)\n",
    "print('Tokenized sentence:\\n',indexed_tokens)\n",
    "print('\\nSegment IDs vector:\\n',segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need Segment IDs? The post https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#what-is-bert explains everything smoothly. Most of the BERT-related tasks of this notebook are based on that tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained BERT and use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d10c0a045a041e5b9aa4ec5d61f1f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to apply the BERT model on one sentence composed of indexes of tokens and segments.\n",
    "def get_hidden_states(bert_model, idx_tokens, idx_segments):\n",
    "    tokens_tensor   = torch.tensor([idx_tokens])\n",
    "    segments_tensor = torch.tensor([idx_segments])\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "    hidden_states = torch.stack(outputs.hidden_states, dim=0) \n",
    "    return hidden_states # Output shape: [layers, batches, seq_len, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BERT hidden states:  torch.Size([13, 1, 7, 768])\n",
      "Num of layers:  13\n",
      "Num batches:  1\n",
      "Sequence length:  7\n",
      "Num of hidden units:  768\n"
     ]
    }
   ],
   "source": [
    "# Apply BERT and analyse the hidden_states.\n",
    "hidden_bert = get_hidden_states(bert_model, indexed_tokens, segments_ids)\n",
    "print('Shape of BERT hidden states: ', hidden_bert.shape)\n",
    "print('Num of layers: ', hidden_bert.shape[0])\n",
    "print('Num batches: ', hidden_bert.shape[1])\n",
    "print('Sequence length: ', hidden_bert.shape[2])\n",
    "print('Num of hidden units: ', hidden_bert.shape[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sentence Embeddings Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the last N layers (last_layers) of each token producing a single N-length vector.\n",
    "def create_sentence_embeddings(hidden_states, layer_idx=-2):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "        hidden_states: output from the BERT model.\n",
    "        layer_idx: the layer to be used for obtaining the features (default=second to last layer).\n",
    "    '''\n",
    "    \n",
    "    # Remove the batch dim, since we work with individual sentences.\n",
    "    token_embeddings = torch.squeeze(hidden_states, dim=1) # Output shape: [layers, seq_len, features]\n",
    "    \n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2) # Output shape: [seq_len, layers, features]\n",
    "    \n",
    "    # Obtain average of features from all tokens.\n",
    "    sentence_embeddings = torch.mean(hidden_states[layer_idx][0], dim=0)\n",
    "    \n",
    "    # Return embeddings.\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function allows us to define features for a sentence using the output of the hidden states of a BERT model. Its output size is equal to $768$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "feature_vector = create_sentence_embeddings(hidden_bert)\n",
    "print(feature_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "Use the `create_sentence_embeddings` function as feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_feature_extractor(sentences, bert_model, preprocess=False, as_numpy=False, layer_idx=-2):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "        sentences: list of sentences.\n",
    "        bert_model: pre-trained BERT model.\n",
    "        as_numpy: return tensors as numpy or not. If False, then dataset[i] is a torch tensor.\n",
    "        \n",
    "    Output\n",
    "        dataset: a list with N tensors (torch or numpy) with features, with N=len(sentences).\n",
    "    '''\n",
    "    \n",
    "    # Our dataset.\n",
    "    dataset = []\n",
    "    \n",
    "    # Put the model in \"evaluation\" mode.\n",
    "    bert_model.eval()\n",
    "    \n",
    "    # Go through the sentences and get the features of each.\n",
    "    for sentence in sentences:\n",
    "        idx_tokens, idx_segments = tokenize_and_segment(sentence, preprocess)\n",
    "        hidden_bert = get_hidden_states(bert_model, idx_tokens, idx_segments)\n",
    "        feature_vector = create_sentence_embeddings(hidden_bert, layer_idx)\n",
    "        dataset.append(feature_vector.numpy() if as_numpy else feature_vector)\n",
    "    \n",
    "    # Return features list.\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(768,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['It is dangerous to go alone, take this.', 'It is raining somewhere else.']\n",
    "dataset = bert_feature_extractor(sentences, bert_model, preprocess=True, as_numpy=True)\n",
    "print(len(dataset))\n",
    "print(dataset[0].shape)\n",
    "print(dataset[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and see the first rows of the dataset.\n",
    "df = pd.read_csv(TRAIN_FILE, sep=\",\")\n",
    "df = df.drop(columns=['id', 'keyword', 'location'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the missing values.\n",
    "missing = df.isna().sum()\n",
    "missing = missing.sort_values(ascending=False)\n",
    "missing = missing[missing>0]\n",
    "missing = pd.DataFrame({'Attribute': missing.index, 'Count': missing.values})\n",
    "print('Number of missing values: %d' % len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets get the counts of each class.\n",
    "counts_class = df.target.value_counts()\n",
    "counts_class = pd.DataFrame({'Class': counts_class.index, 'Count': counts_class.values})\n",
    "\n",
    "# Plot class distribution.\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Class count', fontweight='bold')\n",
    "ax = sns.barplot(data=counts_class, x='Class', y='Count')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.spines['right'].set_visible(False) \n",
    "ax.spines['left'].set_visible(False) \n",
    "ax.spines['top'].set_visible(False) \n",
    "ax.spines['bottom'].set_visible(True) \n",
    "\n",
    "for i in range(0, len(counts_class)):\n",
    "    ax.annotate(counts_class['Count'][i], \n",
    "                xy = (i, counts_class['Count'][i] - 500),\n",
    "                va='center', ha='center', color='#ffff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is some degree of class imbalance, and we can tackle this through different approaches. However, since the difference is not that big, we will train the models using the entire dataset without any strategy to treat class imbalance. We should take one thing into account though, F1-Score must be used as metric to define how well the network performs instead of raw Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract BERT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7613, 768)\n",
      "y_train shape: (7613,)\n"
     ]
    }
   ],
   "source": [
    "# Forward the train set to BERT to obtain the feature vectors for each sentence.\n",
    "X_train = bert_feature_extractor(sentences=df['text'], bert_model=bert_model, preprocess=True, as_numpy=True)\n",
    "y_train = df['target'].tolist()\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (3263, 768)\n"
     ]
    }
   ],
   "source": [
    "# Define a dataframe to contain results and add the predictions.\n",
    "df = pd.read_csv(TEST_FILE, sep=\",\")\n",
    "X_test = bert_feature_extractor(sentences=df['text'], bert_model=bert_model, preprocess=True, as_numpy=True)\n",
    "X_test = np.asarray(X_test)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump pre-processed train and test sets into numpy files.\n",
    "X_TRAIN_BERT_FILE = 'X_train_bert_feat_preprocessed.npy'\n",
    "Y_TRAIN_BERT_FILE = 'y_train_bert_feat_preprocessed.npy'\n",
    "X_TEST_BERT_FILE  = 'X_test_bert_feat_preprocessed.npy'\n",
    "np.save(X_TRAIN_BERT_FILE, X_train)\n",
    "np.save(Y_TRAIN_BERT_FILE, y_train)\n",
    "np.save(X_TEST_BERT_FILE,  X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now that we have our train set prepared, let's define and apply the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class of the model.\n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_units, activations, dropout=0.5):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # Check some errors.\n",
    "        if len(num_units) != len(activations):\n",
    "            print(\"Number of elements in num_units and activations must be the same!\")\n",
    "            return None\n",
    "        # We can also check here if that all elements of num_units are \n",
    "        # two-dimensional, but let's trust the user ok?\n",
    "        \n",
    "        # Actually do the thing.\n",
    "        self.layers      = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        self.num_layers  = len(num_units) \n",
    "        for idx in range(self.num_layers): \n",
    "            self.layers.append(nn.Linear(num_units[idx][0], num_units[idx][1])) # Linear layers.\n",
    "            # Activation functions.\n",
    "            if activations[idx] == 'relu':\n",
    "                self.activations.append(nn.ReLU())\n",
    "            elif activations[idx] == 'sigmoid':\n",
    "                self.activations.append(nn.Sigmoid())\n",
    "            elif activations[idx] == 'softmax':\n",
    "                self.activations.append(nn.Softmax(dim=1))\n",
    "            else:\n",
    "                self.activations.append(None)\n",
    "        self.drop_layer = nn.Dropout(p=dropout) # Dropout layer.\n",
    "        self.init_weights() # Init weights.\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        for idx in range(self.num_layers):\n",
    "            self.layers[idx].weight.data.uniform_(-initrange, initrange)\n",
    "            self.layers[idx].bias.data.zero_()\n",
    "\n",
    "    def forward(self, X, dropout=0.0):\n",
    "        output = X\n",
    "        for idx in range(self.num_layers): # For each layer.\n",
    "            output = self.layers[idx](output) # Linear layers.\n",
    "            if self.activations[idx] != None: # Apply activation function if there is any.\n",
    "                output = self.activations[idx](output)\n",
    "            if idx != self.num_layers - 1: # Apply dropout if it is not the last layer.\n",
    "                output = self.drop_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model for each batch.\n",
    "def train(model, optimizer, criterion, dataloader, epoch):\n",
    "    losses = []\n",
    "    total_acc, total_count, total_loss = 0, 0, 0\n",
    "    log_interval = 50\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted = model(x).view(model(x).shape[0])\n",
    "        loss = criterion(predicted, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # Gradient clipping.\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted.round() == y).sum().item()\n",
    "        total_loss += loss / y.size(0)\n",
    "        total_count += y.size(0)\n",
    "        losses.append((loss / y.size(0)).view(-1).detach().numpy()[0])\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f} | loss {:8.5f}'\n",
    "                  .format(epoch, idx, len(dataloader), total_acc/total_count, total_loss/total_count))\n",
    "            total_acc, total_count, total_loss = 0, 0, 0\n",
    "    return losses # Return a list of the value of the loss at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training settings and load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters.\n",
    "BATCH_SIZE    = 64      # Batch size.\n",
    "EPOCHS        = 50      # Epochs.\n",
    "LEARNING_RATE = 0.0005  # Learning rate.\n",
    "DROPOUT       = 0.3     # Dropout.\n",
    "L2_LAMBDA     = 1e-4    # Lambda for L2 Regularization.\n",
    "\n",
    "# Model config.\n",
    "INPUT_SIZE  = 768 # The size of the BERT features.\n",
    "OUTPUT_SIZE = 1   # Binary classification, so the output is only one element.\n",
    "num_units = [[INPUT_SIZE, 512], [512, 256], [256, OUTPUT_SIZE]]\n",
    "activations = ['relu', 'relu', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with features extracted with BERT.\n",
    "X_TRAIN_BERT_FILE = 'X_train_bert_feat_preprocessed.npy'\n",
    "Y_TRAIN_BERT_FILE = 'y_train_bert_feat_preprocessed.npy'\n",
    "X_train = np.load(X_TRAIN_BERT_FILE)\n",
    "y_train = np.load(Y_TRAIN_BERT_FILE)\n",
    "\n",
    "# Transform to torch tensor.\n",
    "X_train          = torch.Tensor(X_train)\n",
    "y_train          = torch.Tensor(y_train)\n",
    "train_dataset    = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init.\n",
    "model = Classifier(num_units=num_units, activations=activations).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------- TRAINING MODEL --------------------#\n",
      "EPOCH 1 \n",
      "| epoch   1 |    50/  119 batches | accuracy    0.492 | loss  0.00358\n",
      "| epoch   1 |   100/  119 batches | accuracy    0.552 | loss  0.00327\n",
      "EPOCH 2 \n",
      "| epoch   2 |    50/  119 batches | accuracy    0.574 | loss  0.00296\n",
      "| epoch   2 |   100/  119 batches | accuracy    0.611 | loss  0.00283\n",
      "EPOCH 3 \n",
      "| epoch   3 |    50/  119 batches | accuracy    0.650 | loss  0.00248\n",
      "| epoch   3 |   100/  119 batches | accuracy    0.666 | loss  0.00241\n",
      "EPOCH 4 \n",
      "| epoch   4 |    50/  119 batches | accuracy    0.682 | loss  0.00223\n",
      "| epoch   4 |   100/  119 batches | accuracy    0.701 | loss  0.00216\n",
      "EPOCH 5 \n",
      "| epoch   5 |    50/  119 batches | accuracy    0.721 | loss  0.00208\n",
      "| epoch   5 |   100/  119 batches | accuracy    0.718 | loss  0.00199\n",
      "EPOCH 6 \n",
      "| epoch   6 |    50/  119 batches | accuracy    0.738 | loss  0.00193\n",
      "| epoch   6 |   100/  119 batches | accuracy    0.709 | loss  0.00199\n",
      "EPOCH 7 \n",
      "| epoch   7 |    50/  119 batches | accuracy    0.746 | loss  0.00180\n",
      "| epoch   7 |   100/  119 batches | accuracy    0.709 | loss  0.00202\n",
      "EPOCH 8 \n",
      "| epoch   8 |    50/  119 batches | accuracy    0.762 | loss  0.00175\n",
      "| epoch   8 |   100/  119 batches | accuracy    0.737 | loss  0.00190\n",
      "EPOCH 9 \n",
      "| epoch   9 |    50/  119 batches | accuracy    0.758 | loss  0.00173\n",
      "| epoch   9 |   100/  119 batches | accuracy    0.757 | loss  0.00172\n",
      "EPOCH 10 \n",
      "| epoch  10 |    50/  119 batches | accuracy    0.777 | loss  0.00163\n",
      "| epoch  10 |   100/  119 batches | accuracy    0.757 | loss  0.00171\n",
      "EPOCH 11 \n",
      "| epoch  11 |    50/  119 batches | accuracy    0.759 | loss  0.00168\n",
      "| epoch  11 |   100/  119 batches | accuracy    0.762 | loss  0.00167\n",
      "EPOCH 12 \n",
      "| epoch  12 |    50/  119 batches | accuracy    0.784 | loss  0.00162\n",
      "| epoch  12 |   100/  119 batches | accuracy    0.764 | loss  0.00162\n",
      "EPOCH 13 \n",
      "| epoch  13 |    50/  119 batches | accuracy    0.772 | loss  0.00158\n",
      "| epoch  13 |   100/  119 batches | accuracy    0.767 | loss  0.00144\n",
      "EPOCH 14 \n",
      "| epoch  14 |    50/  119 batches | accuracy    0.763 | loss  0.00134\n",
      "| epoch  14 |   100/  119 batches | accuracy    0.765 | loss  0.00120\n",
      "EPOCH 15 \n",
      "| epoch  15 |    50/  119 batches | accuracy    0.777 | loss  0.00090\n",
      "| epoch  15 |   100/  119 batches | accuracy    0.745 | loss  0.00062\n",
      "EPOCH 16 \n",
      "| epoch  16 |    50/  119 batches | accuracy    0.735 | loss  0.00032\n",
      "| epoch  16 |   100/  119 batches | accuracy    0.715 | loss  0.00020\n",
      "EPOCH 17 \n",
      "| epoch  17 |    50/  119 batches | accuracy    0.747 | loss  0.00017\n",
      "| epoch  17 |   100/  119 batches | accuracy    0.753 | loss  0.00014\n",
      "EPOCH 18 \n",
      "| epoch  18 |    50/  119 batches | accuracy    0.777 | loss  0.00013\n",
      "| epoch  18 |   100/  119 batches | accuracy    0.785 | loss  0.00012\n",
      "EPOCH 19 \n",
      "| epoch  19 |    50/  119 batches | accuracy    0.793 | loss  0.00012\n",
      "| epoch  19 |   100/  119 batches | accuracy    0.795 | loss  0.00012\n",
      "EPOCH 20 \n",
      "| epoch  20 |    50/  119 batches | accuracy    0.802 | loss  0.00011\n",
      "| epoch  20 |   100/  119 batches | accuracy    0.805 | loss  0.00011\n",
      "EPOCH 21 \n",
      "| epoch  21 |    50/  119 batches | accuracy    0.803 | loss  0.00011\n",
      "| epoch  21 |   100/  119 batches | accuracy    0.811 | loss  0.00011\n",
      "EPOCH 22 \n",
      "| epoch  22 |    50/  119 batches | accuracy    0.812 | loss  0.00011\n",
      "| epoch  22 |   100/  119 batches | accuracy    0.812 | loss  0.00011\n",
      "EPOCH 23 \n",
      "| epoch  23 |    50/  119 batches | accuracy    0.816 | loss  0.00011\n",
      "| epoch  23 |   100/  119 batches | accuracy    0.821 | loss  0.00010\n",
      "EPOCH 24 \n",
      "| epoch  24 |    50/  119 batches | accuracy    0.811 | loss  0.00010\n",
      "| epoch  24 |   100/  119 batches | accuracy    0.821 | loss  0.00010\n",
      "EPOCH 25 \n",
      "| epoch  25 |    50/  119 batches | accuracy    0.816 | loss  0.00010\n",
      "| epoch  25 |   100/  119 batches | accuracy    0.814 | loss  0.00010\n",
      "EPOCH 26 \n",
      "| epoch  26 |    50/  119 batches | accuracy    0.819 | loss  0.00010\n",
      "| epoch  26 |   100/  119 batches | accuracy    0.825 | loss  0.00010\n",
      "EPOCH 27 \n",
      "| epoch  27 |    50/  119 batches | accuracy    0.817 | loss  0.00010\n",
      "| epoch  27 |   100/  119 batches | accuracy    0.821 | loss  0.00010\n",
      "EPOCH 28 \n",
      "| epoch  28 |    50/  119 batches | accuracy    0.823 | loss  0.00010\n",
      "| epoch  28 |   100/  119 batches | accuracy    0.825 | loss  0.00010\n",
      "EPOCH 29 \n",
      "| epoch  29 |    50/  119 batches | accuracy    0.821 | loss  0.00010\n",
      "| epoch  29 |   100/  119 batches | accuracy    0.828 | loss  0.00010\n",
      "EPOCH 30 \n",
      "| epoch  30 |    50/  119 batches | accuracy    0.824 | loss  0.00010\n",
      "| epoch  30 |   100/  119 batches | accuracy    0.821 | loss  0.00010\n",
      "EPOCH 31 \n",
      "| epoch  31 |    50/  119 batches | accuracy    0.831 | loss  0.00010\n",
      "| epoch  31 |   100/  119 batches | accuracy    0.826 | loss  0.00010\n",
      "EPOCH 32 \n",
      "| epoch  32 |    50/  119 batches | accuracy    0.829 | loss  0.00010\n",
      "| epoch  32 |   100/  119 batches | accuracy    0.831 | loss  0.00009\n",
      "EPOCH 33 \n",
      "| epoch  33 |    50/  119 batches | accuracy    0.832 | loss  0.00010\n",
      "| epoch  33 |   100/  119 batches | accuracy    0.836 | loss  0.00009\n",
      "EPOCH 34 \n",
      "| epoch  34 |    50/  119 batches | accuracy    0.835 | loss  0.00009\n",
      "| epoch  34 |   100/  119 batches | accuracy    0.833 | loss  0.00009\n",
      "EPOCH 35 \n",
      "| epoch  35 |    50/  119 batches | accuracy    0.835 | loss  0.00009\n",
      "| epoch  35 |   100/  119 batches | accuracy    0.837 | loss  0.00009\n",
      "EPOCH 36 \n",
      "| epoch  36 |    50/  119 batches | accuracy    0.832 | loss  0.00009\n",
      "| epoch  36 |   100/  119 batches | accuracy    0.838 | loss  0.00009\n",
      "EPOCH 37 \n",
      "| epoch  37 |    50/  119 batches | accuracy    0.831 | loss  0.00009\n",
      "| epoch  37 |   100/  119 batches | accuracy    0.835 | loss  0.00009\n",
      "EPOCH 38 \n",
      "| epoch  38 |    50/  119 batches | accuracy    0.831 | loss  0.00009\n",
      "| epoch  38 |   100/  119 batches | accuracy    0.849 | loss  0.00009\n",
      "EPOCH 39 \n",
      "| epoch  39 |    50/  119 batches | accuracy    0.844 | loss  0.00009\n",
      "| epoch  39 |   100/  119 batches | accuracy    0.847 | loss  0.00009\n",
      "EPOCH 40 \n",
      "| epoch  40 |    50/  119 batches | accuracy    0.848 | loss  0.00009\n",
      "| epoch  40 |   100/  119 batches | accuracy    0.843 | loss  0.00008\n",
      "EPOCH 41 \n",
      "| epoch  41 |    50/  119 batches | accuracy    0.853 | loss  0.00008\n",
      "| epoch  41 |   100/  119 batches | accuracy    0.853 | loss  0.00008\n",
      "EPOCH 42 \n",
      "| epoch  42 |    50/  119 batches | accuracy    0.855 | loss  0.00008\n",
      "| epoch  42 |   100/  119 batches | accuracy    0.859 | loss  0.00008\n",
      "EPOCH 43 \n",
      "| epoch  43 |    50/  119 batches | accuracy    0.859 | loss  0.00008\n",
      "| epoch  43 |   100/  119 batches | accuracy    0.858 | loss  0.00008\n",
      "EPOCH 44 \n",
      "| epoch  44 |    50/  119 batches | accuracy    0.858 | loss  0.00008\n",
      "| epoch  44 |   100/  119 batches | accuracy    0.864 | loss  0.00008\n",
      "EPOCH 45 \n",
      "| epoch  45 |    50/  119 batches | accuracy    0.860 | loss  0.00008\n",
      "| epoch  45 |   100/  119 batches | accuracy    0.861 | loss  0.00008\n",
      "EPOCH 46 \n",
      "| epoch  46 |    50/  119 batches | accuracy    0.873 | loss  0.00007\n",
      "| epoch  46 |   100/  119 batches | accuracy    0.863 | loss  0.00008\n",
      "EPOCH 47 \n",
      "| epoch  47 |    50/  119 batches | accuracy    0.862 | loss  0.00008\n",
      "| epoch  47 |   100/  119 batches | accuracy    0.863 | loss  0.00007\n",
      "EPOCH 48 \n",
      "| epoch  48 |    50/  119 batches | accuracy    0.869 | loss  0.00007\n",
      "| epoch  48 |   100/  119 batches | accuracy    0.875 | loss  0.00007\n",
      "EPOCH 49 \n",
      "| epoch  49 |    50/  119 batches | accuracy    0.870 | loss  0.00007\n",
      "| epoch  49 |   100/  119 batches | accuracy    0.871 | loss  0.00007\n",
      "EPOCH 50 \n",
      "| epoch  50 |    50/  119 batches | accuracy    0.876 | loss  0.00007\n",
      "| epoch  50 |   100/  119 batches | accuracy    0.880 | loss  0.00007\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer.\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
    "\n",
    "# Train the network.\n",
    "losses_per_epoch = []\n",
    "model.train()\n",
    "print('#' + '-' * 20 + ' TRAINING MODEL ' + '-' * 20 + '#')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print('EPOCH %d ' % epoch)\n",
    "    loss = train(model, optimizer, criterion, train_dataloader, epoch)\n",
    "    losses_per_epoch.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "all_losses  = []\n",
    "epoch = []\n",
    "loss_val = []\n",
    "for i in range(0, len(losses_per_epoch)):\n",
    "    # To plot all losses.\n",
    "    all_losses.extend(losses_per_epoch[i])\n",
    "    # To plot the average loss of each epoch.\n",
    "    epoch.append(i*len(losses_per_epoch[i])) \n",
    "    loss_val.append(np.mean(losses_per_epoch[i]))\n",
    "#plt.plot(all_losses)\n",
    "plt.plot(epoch, loss_val, \"-\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the figure it seems that the network falls into a local minimum a little bit after iteration 1000. We will try to solve this in the future through some regularization techniques as well as optimizing the network's hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to forward the trained model and write report.\n",
    "def get_model_predictions(model, dataset):\n",
    "    model.eval()\n",
    "    y_probs  = []\n",
    "    y_preds  = []\n",
    "    with torch.no_grad():\n",
    "        for idx, x in enumerate(dataset):\n",
    "            probs = model(x).numpy()\n",
    "            y_probs.append(probs[0])\n",
    "            y_preds.append(int(round(probs[0])))\n",
    "    return y_probs, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with features extracted with BERT and define dataloader.\n",
    "X_TEST_BERT_FILE  = 'X_test_bert_feat_preprocessed.npy'\n",
    "X_test           = np.load(X_TEST_BERT_FILE)\n",
    "X_test           = torch.Tensor(X_test)\n",
    "\n",
    "# Forward.\n",
    "y_probs, y_preds = get_model_predictions(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format results.\n",
    "df = pd.read_csv(TEST_FILE, sep=\",\")\n",
    "df = df.drop(columns=['text', 'keyword', 'location'])\n",
    "df['target'] = y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Names of files.\n",
    "RESULTS_FILE = 'bert_nn_test_results.csv'\n",
    "\n",
    "# Save the results and the best model.\n",
    "df.to_csv(RESULTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results in the Kaggle competition\n",
    "This model achieved a score equal to $0.81152$. Not bad, but also not perfect. I will try to improve this in the future. For instance, note that there is no validation split, this could be done to perform hyperparameter tuning of the network. Also note that the original dataset also contains two more attributes: keyword and location. These could be also included as input to the model to see if they can provide information to improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
